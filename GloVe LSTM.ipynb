{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa8bc49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:01:26.089579Z",
     "iopub.status.busy": "2023-08-04T11:01:26.088410Z",
     "iopub.status.idle": "2023-08-04T11:01:40.797035Z",
     "shell.execute_reply": "2023-08-04T11:01:40.795897Z",
     "shell.execute_reply.started": "2023-08-04T11:01:26.089512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import random\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import compounding\n",
    "from spacy.util import minibatch\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import (LSTM, \n",
    "                          Embedding, \n",
    "                          BatchNormalization,\n",
    "                          Dense, \n",
    "                          TimeDistributed, \n",
    "                          Dropout, \n",
    "                          Bidirectional,\n",
    "                          Flatten, \n",
    "                          GlobalMaxPool1D)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "##from keras.preprocessing.sequence import pad_sequences\n",
    "##from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    classification_report,\n",
    "    accuracy_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706deb4",
   "metadata": {},
   "source": [
    "#  LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f0362c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:01:47.835341Z",
     "iopub.status.busy": "2023-08-04T11:01:47.834605Z",
     "iopub.status.idle": "2023-08-04T11:01:47.945875Z",
     "shell.execute_reply": "2023-08-04T11:01:47.944921Z",
     "shell.execute_reply.started": "2023-08-04T11:01:47.835307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target  text_len\n",
       "0   1  Our Deeds are the Reason of this #earthquake M...       1        13\n",
       "1   4             Forest fire near La Ronge Sask. Canada       1         7\n",
       "2   5  All residents asked to 'shelter in place' are ...       1        22\n",
       "3   6  13,000 people receive #wildfires evacuation or...       1         9\n",
       "4   7  Just got sent this photo from Ruby #Alaska as ...       1        17"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\", encoding=\"latin-1\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\", encoding=\"latin-1\")\n",
    "\n",
    "df = df.dropna(how=\"any\", axis=1)\n",
    "df['text_len'] = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c31a01c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:01:48.858862Z",
     "iopub.status.busy": "2023-08-04T11:01:48.858214Z",
     "iopub.status.idle": "2023-08-04T11:01:48.873412Z",
     "shell.execute_reply": "2023-08-04T11:01:48.872286Z",
     "shell.execute_reply.started": "2023-08-04T11:01:48.858828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4342, 3271])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_counts = df.groupby('target')['target'].agg('count').values\n",
    "balance_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49adb22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:01:49.764509Z",
     "iopub.status.busy": "2023-08-04T11:01:49.764146Z",
     "iopub.status.idle": "2023-08-04T11:01:49.774596Z",
     "shell.execute_reply": "2023-08-04T11:01:49.773308Z",
     "shell.execute_reply.started": "2023-08-04T11:01:49.764482Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', text)\n",
    "\n",
    "# Special thanks to https://www.kaggle.com/tanulsingh077 for this function\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\n",
    "        'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \n",
    "        '', \n",
    "        text\n",
    "    )\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    text = remove_url(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_html(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1ddf7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:01:51.174524Z",
     "iopub.status.busy": "2023-08-04T11:01:51.173421Z",
     "iopub.status.idle": "2023-08-04T11:01:51.184334Z",
     "shell.execute_reply": "2023-08-04T11:01:51.183506Z",
     "shell.execute_reply.started": "2023-08-04T11:01:51.174486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Omg another Earthquake '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test emoji removal\n",
    "remove_emoji(\"Omg another Earthquake 😔😔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5667b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:01:51.971187Z",
     "iopub.status.busy": "2023-08-04T11:01:51.970488Z",
     "iopub.status.idle": "2023-08-04T11:01:51.982121Z",
     "shell.execute_reply": "2023-08-04T11:01:51.981080Z",
     "shell.execute_reply.started": "2023-08-04T11:01:51.971149Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "more_stopwords = ['u', 'im', 'c']\n",
    "stop_words = stop_words + more_stopwords\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "def preprocess_data(text):\n",
    "    # Clean puntuation, urls, and so on\n",
    "    text = clean_text(text)\n",
    "    # Remove stopwords and Stemm all the words in the sentence\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' ') if word not in stop_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "722e6b83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:01:53.312850Z",
     "iopub.status.busy": "2023-08-04T11:01:53.312258Z",
     "iopub.status.idle": "2023-08-04T11:01:55.703672Z",
     "shell.execute_reply": "2023-08-04T11:01:55.702549Z",
     "shell.execute_reply.started": "2023-08-04T11:01:53.312804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target  text_len  \\\n",
       "0   1  Our Deeds are the Reason of this #earthquake M...       1        13   \n",
       "1   4             Forest fire near La Ronge Sask. Canada       1         7   \n",
       "2   5  All residents asked to 'shelter in place' are ...       1        22   \n",
       "3   6  13,000 people receive #wildfires evacuation or...       1         9   \n",
       "4   7  Just got sent this photo from Ruby #Alaska as ...       1        17   \n",
       "\n",
       "                                          text_clean  \n",
       "0          deed reason earthquak may allah forgiv us  \n",
       "1               forest fire near la rong sask canada  \n",
       "2  resid ask shelter place notifi offic evacu she...  \n",
       "3       peopl receiv wildfir evacu order california   \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['text_clean'] = test_df['text'].apply(preprocess_data)\n",
    "\n",
    "df['text_clean'] = df['text'].apply(preprocess_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b20af4be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:01:55.706165Z",
     "iopub.status.busy": "2023-08-04T11:01:55.705754Z",
     "iopub.status.idle": "2023-08-04T11:01:55.711325Z",
     "shell.execute_reply": "2023-08-04T11:01:55.710288Z",
     "shell.execute_reply.started": "2023-08-04T11:01:55.706124Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_corpus_df(tweet, target):\n",
    "    corpus=[]\n",
    "    \n",
    "    for x in tweet[tweet['target']==target]['text_clean'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cae528f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:01:57.153244Z",
     "iopub.status.busy": "2023-08-04T11:01:57.152875Z",
     "iopub.status.idle": "2023-08-04T11:01:57.185390Z",
     "shell.execute_reply": "2023-08-04T11:01:57.184325Z",
     "shell.execute_reply.started": "2023-08-04T11:01:57.153216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fire', 266),\n",
       " ('bomb', 179),\n",
       " ('kill', 158),\n",
       " ('news', 132),\n",
       " ('via', 121),\n",
       " ('flood', 120),\n",
       " ('disast', 116),\n",
       " ('california', 115),\n",
       " ('crash', 110),\n",
       " ('suicid', 110)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_disaster_tweets = create_corpus_df(df, 1)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus_disaster_tweets:\n",
    "    dic[word]+=1\n",
    "        \n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3e2967c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:01:58.134149Z",
     "iopub.status.busy": "2023-08-04T11:01:58.133504Z",
     "iopub.status.idle": "2023-08-04T11:01:58.170490Z",
     "shell.execute_reply": "2023-08-04T11:01:58.169432Z",
     "shell.execute_reply.started": "2023-08-04T11:01:58.134113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 306),\n",
       " ('get', 222),\n",
       " ('amp', 192),\n",
       " ('new', 168),\n",
       " ('go', 142),\n",
       " ('dont', 139),\n",
       " ('one', 134),\n",
       " ('bodi', 116),\n",
       " ('love', 115),\n",
       " ('bag', 108)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_disaster_tweets = create_corpus_df(df, 0)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus_disaster_tweets:\n",
    "    dic[word]+=1\n",
    "        \n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e3a6128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:02:22.661420Z",
     "iopub.status.busy": "2023-08-04T11:02:22.661009Z",
     "iopub.status.idle": "2023-08-04T11:02:22.671978Z",
     "shell.execute_reply": "2023-08-04T11:02:22.670933Z",
     "shell.execute_reply.started": "2023-08-04T11:02:22.661387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5709 5709\n",
      "1904 1904\n"
     ]
    }
   ],
   "source": [
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "x = df['text_clean']\n",
    "y = df['target']\n",
    "\n",
    "# Split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b09bd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:02:23.489690Z",
     "iopub.status.busy": "2023-08-04T11:02:23.488881Z",
     "iopub.status.idle": "2023-08-04T11:02:23.496227Z",
     "shell.execute_reply": "2023-08-04T11:02:23.494956Z",
     "shell.execute_reply.started": "2023-08-04T11:02:23.489647Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tweets = df['text_clean'].values\n",
    "test_tweets = test_df['text_clean'].values\n",
    "train_target = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe10416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:02:32.517115Z",
     "iopub.status.busy": "2023-08-04T11:02:32.516065Z",
     "iopub.status.idle": "2023-08-04T11:02:32.670641Z",
     "shell.execute_reply": "2023-08-04T11:02:32.669430Z",
     "shell.execute_reply.started": "2023-08-04T11:02:32.517074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13704"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the length of our vocabulary\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(train_tweets)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b39c41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:02:33.492448Z",
     "iopub.status.busy": "2023-08-04T11:02:33.492101Z",
     "iopub.status.idle": "2023-08-04T11:02:33.498375Z",
     "shell.execute_reply": "2023-08-04T11:02:33.497258Z",
     "shell.execute_reply.started": "2023-08-04T11:02:33.492421Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_metrics(pred_tag, y_test):\n",
    "    print(\"F1-score: \", f1_score(pred_tag, y_test))\n",
    "    print(\"Precision: \", precision_score(pred_tag, y_test))\n",
    "    print(\"Recall: \", recall_score(pred_tag, y_test))\n",
    "    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n",
    "    print(\"-\"*50)\n",
    "    print(classification_report(pred_tag, y_test))\n",
    "    \n",
    "def embed(corpus): \n",
    "    return word_tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b702bac6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:02:34.409281Z",
     "iopub.status.busy": "2023-08-04T11:02:34.408881Z",
     "iopub.status.idle": "2023-08-04T11:02:35.449891Z",
     "shell.execute_reply": "2023-08-04T11:02:35.448919Z",
     "shell.execute_reply.started": "2023-08-04T11:02:34.409248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3635,  467,  201, ...,    0,    0,    0],\n",
       "       [ 136,    2,  106, ...,    0,    0,    0],\n",
       "       [1338,  502, 1807, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 448, 1328,    0, ...,    0,    0,    0],\n",
       "       [  28,  162, 2637, ...,    0,    0,    0],\n",
       "       [ 171,   31,  413, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_train = max(train_tweets, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "\n",
    "train_padded_sentences = pad_sequences(\n",
    "    embed(train_tweets), \n",
    "    length_long_sentence, \n",
    "    padding='post'\n",
    ")\n",
    "test_padded_sentences = pad_sequences(\n",
    "    embed(test_tweets), \n",
    "    length_long_sentence,\n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "train_padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46ac7b45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:02:36.233422Z",
     "iopub.status.busy": "2023-08-04T11:02:36.232535Z",
     "iopub.status.idle": "2023-08-04T11:02:49.872406Z",
     "shell.execute_reply": "2023-08-04T11:02:49.867895Z",
     "shell.execute_reply.started": "2023-08-04T11:02:36.233382Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "embedding_dim = 100\n",
    "\n",
    "# Load GloVe 100D embeddings\n",
    "with open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt') as fp:\n",
    "    for line in fp.readlines():\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "# embeddings_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3665b9ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:03:16.145318Z",
     "iopub.status.busy": "2023-08-04T11:03:16.144926Z",
     "iopub.status.idle": "2023-08-04T11:03:16.178539Z",
     "shell.execute_reply": "2023-08-04T11:03:16.177617Z",
     "shell.execute_reply.started": "2023-08-04T11:03:16.145285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.2687    ,  0.81708002,  0.69896001, ..., -0.40110001,\n",
       "         0.74656999,  0.31121999],\n",
       "       [-0.26872   , -0.15542001, -0.23565   , ...,  0.49344   ,\n",
       "         0.72114003, -0.041749  ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.19814   , -0.33517   , -0.13950001, ..., -0.070356  ,\n",
       "        -0.18391   ,  0.62439001],\n",
       "       [-0.34132001,  0.26423001,  0.47813001, ..., -0.92395002,\n",
       "         0.48275   ,  0.52947998]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will load embedding vectors of those words that appear in the\n",
    "# Glove dictionary. Others will be initialized to 0.\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        \n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9015676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:03:17.003618Z",
     "iopub.status.busy": "2023-08-04T11:03:17.002954Z",
     "iopub.status.idle": "2023-08-04T11:03:17.503516Z",
     "shell.execute_reply": "2023-08-04T11:03:17.502491Z",
     "shell.execute_reply.started": "2023-08-04T11:03:17.003577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 23, 100)           1370400   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 23, 46)           22816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 46)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 46)               184       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 46)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 23)                1081      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 23)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 23)                552       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 23)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 24        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,395,057\n",
      "Trainable params: 1,394,965\n",
      "Non-trainable params: 92\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def glove_lstm():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(\n",
    "        input_dim=embedding_matrix.shape[0], \n",
    "        output_dim=embedding_matrix.shape[1], \n",
    "        weights = [embedding_matrix], \n",
    "        input_length=length_long_sentence\n",
    "    ))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(\n",
    "        length_long_sentence, \n",
    "        return_sequences = True, \n",
    "        recurrent_dropout=0.2\n",
    "    )))\n",
    "    \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = glove_lstm()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fc25eaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:03:20.104470Z",
     "iopub.status.busy": "2023-08-04T11:03:20.104108Z",
     "iopub.status.idle": "2023-08-04T11:03:20.109778Z",
     "shell.execute_reply": "2023-08-04T11:03:20.108689Z",
     "shell.execute_reply.started": "2023-08-04T11:03:20.104442Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_padded_sentences, \n",
    "    train_target, \n",
    "    test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8e9ce40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T11:05:01.057209Z",
     "iopub.status.busy": "2023-08-04T11:05:01.056809Z",
     "iopub.status.idle": "2023-08-04T11:05:45.902260Z",
     "shell.execute_reply": "2023-08-04T11:05:45.901150Z",
     "shell.execute_reply.started": "2023-08-04T11:05:01.057176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.8838 - accuracy: 0.5467\n",
      "Epoch 1: val_loss improved from inf to 0.67341, saving model to model.h5\n",
      "179/179 [==============================] - 11s 34ms/step - loss: 0.8838 - accuracy: 0.5467 - val_loss: 0.6734 - val_accuracy: 0.5725 - lr: 0.0010\n",
      "Epoch 2/7\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.6723 - accuracy: 0.6301\n",
      "Epoch 2: val_loss improved from 0.67341 to 0.60285, saving model to model.h5\n",
      "179/179 [==============================] - 5s 30ms/step - loss: 0.6723 - accuracy: 0.6301 - val_loss: 0.6028 - val_accuracy: 0.7243 - lr: 0.0010\n",
      "Epoch 3/7\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.5916 - accuracy: 0.7022\n",
      "Epoch 3: val_loss improved from 0.60285 to 0.51667, saving model to model.h5\n",
      "179/179 [==============================] - 6s 31ms/step - loss: 0.5916 - accuracy: 0.7022 - val_loss: 0.5167 - val_accuracy: 0.7532 - lr: 0.0010\n",
      "Epoch 4/7\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.5444 - accuracy: 0.7502\n",
      "Epoch 4: val_loss improved from 0.51667 to 0.48527, saving model to model.h5\n",
      "179/179 [==============================] - 5s 30ms/step - loss: 0.5444 - accuracy: 0.7502 - val_loss: 0.4853 - val_accuracy: 0.7773 - lr: 0.0010\n",
      "Epoch 5/7\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.5002 - accuracy: 0.7788\n",
      "Epoch 5: val_loss improved from 0.48527 to 0.48251, saving model to model.h5\n",
      "179/179 [==============================] - 6s 31ms/step - loss: 0.5002 - accuracy: 0.7788 - val_loss: 0.4825 - val_accuracy: 0.7589 - lr: 0.0010\n",
      "Epoch 6/7\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.8007\n",
      "Epoch 6: val_loss improved from 0.48251 to 0.46738, saving model to model.h5\n",
      "179/179 [==============================] - 6s 32ms/step - loss: 0.4752 - accuracy: 0.8007 - val_loss: 0.4674 - val_accuracy: 0.7684 - lr: 0.0010\n",
      "Epoch 7/7\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4496 - accuracy: 0.8198\n",
      "Epoch 7: val_loss improved from 0.46738 to 0.45771, saving model to model.h5\n",
      "179/179 [==============================] - 6s 33ms/step - loss: 0.4496 - accuracy: 0.8198 - val_loss: 0.4577 - val_accuracy: 0.7889 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model = glove_lstm()\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5', \n",
    "    monitor = 'val_loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss', \n",
    "    factor = 0.2, \n",
    "    verbose = 1, \n",
    "    patience = 5,                        \n",
    "    min_lr = 0.001\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs = 7,\n",
    "    batch_size = 32,\n",
    "    validation_data = (X_test, y_test),\n",
    "    verbose = 1,\n",
    "    callbacks = [reduce_lr, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07898bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
